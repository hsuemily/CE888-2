{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset3 SafeDriver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOj2T3oGneIAJKTsz9Yv6px",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsuemily/CE888_Hsu-Chi-Rou_1900759/blob/master/Assignment/Dataset3_SafeDriver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2MbAQjr16Ag",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWjejzz13Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLYX_J-qSlOE",
        "colab_type": "text"
      },
      "source": [
        "## 2.  Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcrjJN8bzDfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "# Colab library to upload files to notebook, Ref:https://medium.com/@saedhussain/google-colaboratory-and-kaggle-datasets-b57a83eb6ef8\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Load data from Kaggle to Google Colab virtual machine\n",
        "os.environ['KAGGLE_USERNAME'] = \"emilyhsucr\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"3b6aa5aa02b468c89e4cfac52a5d992e\" # key from the json file\n",
        "!kaggle competitions download -c porto-seguro-safe-driver-prediction # api copied from kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmK4BGDPo8M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Porto Seguroâ€™s Safe Driver Prediction \n",
        "SafeDriver = pd.read_csv('train.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO6CzvuwzMA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SafeDriver.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZDpwT06Zjqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SafeDriver.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEIQxqqP2P9B",
        "colab_type": "text"
      },
      "source": [
        "## 3. Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX2mj4SrpOjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot histogram of the label target in Safe Driver dataset\n",
        "pd.Series(SafeDriver['target']).value_counts().plot(kind='bar',title='SafeDriver: Count (target)')\n",
        "target_count = SafeDriver.target.value_counts()\n",
        "print('will not initiate an auto insurance claim (0):', target_count[0])\n",
        "print('will initiate an auto insurance claim (1):', target_count[1])\n",
        "print('Imbalance rate:', round((target_count[0] / (target_count[0]+target_count[1]))*100, 2), '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7sgM8Bl_Ddi",
        "colab_type": "text"
      },
      "source": [
        "## 4. Data Preprocessing\n",
        " - 4.1 Missing Value\n",
        " - 4.2 Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clG5rNwDSz5K",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3N6VzM3_J-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dealing with missing value\n",
        "\n",
        "#*****SafeDriver\n",
        "total = SafeDriver.isnull().sum().sort_values(ascending=False)\n",
        "# summarize the amount of missing value in each attributes, and then sort it by descending order\n",
        "# percent = the counts of missing value /the counts of item (in each attribute)\n",
        "percent = (SafeDriver.isnull().sum()/SafeDriver.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n",
        "missing_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0n0-wD_S-o3",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpgg7-FTLg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize dataset - SafeDriver\n",
        "SafeDriver_copy = SafeDriver.copy()\n",
        "for col in SafeDriver_copy.select_dtypes(include='number').columns:\n",
        "    mms = MinMaxScaler()\n",
        "    SafeDriver_copy[col] = mms.fit_transform(SafeDriver_copy[[col]])\n",
        "SafeDriver_copy.describe()\n",
        "\n",
        "# separate the data in to x and y\n",
        "SafeDriver_X = SafeDriver_copy.drop('target', axis = 1)\n",
        "SafeDriver_X=SafeDriver_X.values\n",
        "SafeDriver_Y = np.array(SafeDriver_copy['target'])\n",
        "print(\"SafeDriver:\",SafeDriver_X)\n",
        "print(SafeDriver_X.shape)\n",
        "print(SafeDriver_Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZGuHw67IS4o",
        "colab_type": "text"
      },
      "source": [
        "## 5. Supervised Learning\n",
        "### 5.1 Decision tree (cross-validation)\n",
        "### 5.2 Random forest (cross-validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-a_ILqfIkzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Decision tree \n",
        "\n",
        "### Ref_1: https://www.kaggle.com/sudhirnl7/logistic-regression-with-stratifiedkfold\n",
        "### Ref_2: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#\n",
        "#Import library\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "Skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=49)\n",
        "pred_test_full =0\n",
        "cv_percisionDT =[]\n",
        "cv_recallDT =[]\n",
        "cv_F1scoreDT =[]\n",
        "cv_scoreDTRoc = []\n",
        "i=1\n",
        "for train_index,test_index in Skf.split(SafeDriver_X,SafeDriver_Y):\n",
        "    print('{} of KFold {}'.format(i,Skf.n_splits))\n",
        "    x_tr,x_v = SafeDriver_X[train_index],SafeDriver_X[test_index]\n",
        "    y_tr,y_v = SafeDriver_Y[train_index],SafeDriver_Y[test_index]\n",
        "    \n",
        "    #model\n",
        "    modelDT = tree.DecisionTreeClassifier(criterion = \"entropy\",random_state=1)\n",
        "    # model = tree.DecisionTreeClassifier(max_depth=5,max_leaf_nodes=15, max_features=14, criterion = \"entropy\",random_state=1)\n",
        "    modelDT.fit(x_tr,y_tr)\n",
        "    percisionDT=precision_score(y_v,modelDT.predict(x_v))\n",
        "    recallDT=recall_score(y_v,modelDT.predict(x_v))\n",
        "    F1scoreDT=f1_score(y_v,modelDT.predict(x_v))\n",
        "    scoreDTRoc = roc_auc_score(y_v,modelDT.predict(x_v))\n",
        "\n",
        "    cv_percisionDT.append(percisionDT)\n",
        "    cv_recallDT.append(recallDT)\n",
        "    cv_F1scoreDT.append(F1scoreDT)\n",
        "    cv_scoreDTRoc.append(scoreDTRoc)\n",
        "\n",
        "    print('Confusion matrix\\n',confusion_matrix(y_v,modelDT.predict(x_v)))\n",
        "    print('Precision:', precision_score(y_v,modelDT.predict(x_v)))\n",
        "    print('Recall:',recall_score(y_v,modelDT.predict(x_v)))\n",
        "    print('F1 score:', f1_score(y_v,modelDT.predict(x_v)))\n",
        "    print('ROC AUC score:',scoreDTRoc)\n",
        "\n",
        "    i+=1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUq3fEqskqe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statistics import mean, stdev\n",
        "print(\"Mean of percision:\", round(mean(cv_percisionDT),3), '\\nStandard Deviation of percision:', round(stdev(cv_percisionDT),3))\n",
        "print(\"Mean of recall:\", round(mean(cv_recallDT),3), '\\nStandard Deviation of recall:', round(stdev(cv_recallDT),3))\n",
        "print(\"Mean of F1score:\", round(mean(cv_F1scoreDT),3), '\\nStandard Deviation of F1score:', round(stdev(cv_F1scoreDT),3))\n",
        "print(\"Mean of Roc score:\", round(mean(cv_scoreDTRoc),3), '\\nStandard Deviation of Roc score:', round(stdev(cv_scoreDTRoc),3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD_uevDfoEFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "cv_percisionRF =[]\n",
        "cv_recallRF =[]\n",
        "cv_F1scoreRF =[]\n",
        "cv_scoreRFRoc = []\n",
        "i=1\n",
        "for train_index,test_index in Skf.split(SafeDriver_X,SafeDriver_Y):\n",
        "    print('{} of KFold {}'.format(i,Skf.n_splits))\n",
        "    x_tr,x_v = SafeDriver_X[train_index],SafeDriver_X[test_index]\n",
        "    y_tr,y_v = SafeDriver_Y[train_index],SafeDriver_Y[test_index]\n",
        "    \n",
        "    #model\n",
        "    modelRF = RandomForestClassifier(random_state=1)\n",
        "    modelRF.fit(x_tr,y_tr)\n",
        "\n",
        "    percisionRF=precision_score(y_v,modelRF.predict(x_v))\n",
        "    recallRF=recall_score(y_v,modelRF.predict(x_v))\n",
        "    F1scoreRF=f1_score(y_v,modelRF.predict(x_v))\n",
        "    scoreRFRoc = roc_auc_score(y_v,modelRF.predict(x_v))\n",
        "\n",
        "    cv_percisionRF.append(percisionRF)\n",
        "    cv_recallRF.append(recallRF)\n",
        "    cv_F1scoreRF.append(F1scoreRF)\n",
        "    cv_scoreRFRoc.append(scoreRFRoc)\n",
        "\n",
        "    print('Confusion matrix\\n',confusion_matrix(y_v,modelRF.predict(x_v)))\n",
        "    print('Precision:', precision_score(y_v,modelRF.predict(x_v)))\n",
        "    print('Recall:',recall_score(y_v,modelRF.predict(x_v)))\n",
        "    print('F1 score:', f1_score(y_v,modelRF.predict(x_v)))\n",
        "    print('ROC AUC score:',scoreRFRoc)\n",
        "\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nGPNhpJs5fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statistics import mean, stdev\n",
        "print(\"Mean of percision:\", round(mean(cv_percisionRF),3), '\\nStandard Deviation of percision:', round(stdev(cv_percisionRF),3))\n",
        "print(\"Mean of recall:\", round(mean(cv_recallRF),3), '\\nStandard Deviation of recall:', round(stdev(cv_recallRF),3))\n",
        "print(\"Mean of F1score:\", round(mean(cv_F1scoreRF),3), '\\nStandard Deviation of F1score:', round(stdev(cv_F1scoreRF),3))\n",
        "print(\"Mean of Roc score:\", round(mean(cv_scoreRFRoc),3), '\\nStandard Deviation of Roc score:', round(stdev(cv_scoreRFRoc),3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Co-aghfwvrV",
        "colab_type": "text"
      },
      "source": [
        "## 6. Unsupervised Learning\n",
        "6.1  Using the Elbow method and the Silhouette method, identify the number of clusters in the dataset.\n",
        "\n",
        "6.2 K-mean method and save the information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lcTxf3gvUKw",
        "colab_type": "text"
      },
      "source": [
        "### 6.1 Using the Elbow method and the Silhouette method, identify the number of clusters in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSrHmN-ySqgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Elbow method (K-mean method) (stratified cross-validation)\n",
        "### Ref: https://www.kaggle.com/abhishekyadav5/kmeans-clustering-with-elbow-method-and-silhouette\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "Skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=49)\n",
        "pred_test_full =0\n",
        "i=1\n",
        "scoreE1_all=[]\n",
        "for train_index,test_index in Skf.split(SafeDriver_X,SafeDriver_Y):\n",
        "    print('{} of KFold {}'.format(i,Skf.n_splits))\n",
        "    x_tr,x_v = SafeDriver_X[train_index],SafeDriver_X[test_index]\n",
        "    y_tr,y_v = SafeDriver_Y[train_index],SafeDriver_Y[test_index]\n",
        "\n",
        "    scoreEl = []\n",
        "    for cluster in range(1,9):\n",
        "        kmeans = KMeans(n_clusters = cluster, init=\"k-means++\", random_state=49)\n",
        "        kmeans.fit(x_tr)\n",
        "        scoreEl.append(kmeans.inertia_)\n",
        "    scoreE1_all.append(scoreEl)\n",
        "\n",
        "    # plotting the score\n",
        "\n",
        "    plt.plot(range(1,9), scoreEl, 'g-o')\n",
        "    plt.title('The Elbow Method')\n",
        "    plt.xlabel('no of clusters')\n",
        "    plt.ylabel('Total within-cluster sum of square')\n",
        "    plt.show()\n",
        "    i+=1\n",
        "    ## Total within-cluster sum of square: https://www.jamleecute.com/partitional-clustering-kmeans-kmedoid/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGdxUUzW3nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Silhouette score\n",
        "### Ref_1: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "### Ref_2: https://www.kaggle.com/abhishekyadav5/kmeans-clustering-with-elbow-method-and-silhouette\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "silhouette_avg_list = []\n",
        "n_clusters_list = []\n",
        "for n_clusters in range(2,9):\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(x_tr) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=49)\n",
        "    cluster_labels = clusterer.fit_predict(x_tr)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(x_tr, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "    silhouette_avg_list += [silhouette_avg]\n",
        "    n_clusters_list += [n_clusters]\n",
        "    \n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(x_tr, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(x_tr[:, 2], x_tr[:, 8], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 2], centers[:, 8], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[2], c[8], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 3rd feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 9th feature\")\n",
        "    # ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    # ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX1e7e7oHv6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(n_clusters_list, silhouette_avg_list, 'g-o')\n",
        "plt.title('The Silhouette method')\n",
        "plt.xlabel('no of clusters')\n",
        "plt.ylabel('The Silhouette score')\n",
        "plt.show()\n",
        "print(silhouette_avg_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ05FTkQvlqF",
        "colab_type": "text"
      },
      "source": [
        "### 6.2 K-mean method and save the information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPbedWIH5Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Ref: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=49).fit(x_tr)\n",
        "kmLabels = kmeans.labels_\n",
        "Label = [np.count_nonzero(kmLabels==0),np.count_nonzero(kmLabels==1)]\n",
        "centroids = kmeans.cluster_centers_\n",
        "print('Label:', Label)\n",
        "print('Centroids',centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXEXg3nyMVHp",
        "colab_type": "text"
      },
      "source": [
        "## 7. Mixture Method\n",
        "A new approach to dealing with imbalanced datasets, based on a mixture of\n",
        "supervised and unsupervised learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAzGALWJnRfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Partition each of the datasets into 10 bins\n",
        "## , keeping the imbalance ratio from the original dataset \n",
        "## Ref: https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
        "\n",
        "Skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=49)\n",
        "pred_test_full =0\n",
        "Label_all = []\n",
        "centroids_all = []\n",
        "\n",
        "cv_percisionRFnew =[]\n",
        "cv_recallRFnew =[]\n",
        "cv_F1scoreRFnew =[]\n",
        "cv_scoreRFRocnew = []\n",
        "\n",
        "ConfusionMatrix_all = []\n",
        "\n",
        "i=1\n",
        "\n",
        "## Use different 9 bins in training dataset,\n",
        "## and remain one to be the testing dataset\n",
        "for train_index,test_index in Skf.split(SafeDriver_X,SafeDriver_Y):\n",
        "    print('{} of KFold {}'.format(i,Skf.n_splits))\n",
        "    x_tr,x_v = SafeDriver_X[train_index],SafeDriver_X[test_index]\n",
        "    y_tr,y_v = SafeDriver_Y[train_index],SafeDriver_Y[test_index]\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=2, random_state=49).fit(x_tr)\n",
        "    kmLabels = kmeans.labels_\n",
        "    ## save the centroid and the number of samples in each cluster in each 9 bins\n",
        "    Label = [np.count_nonzero(kmLabels==0),np.count_nonzero(kmLabels==1)]\n",
        "    Label_all += [Label]\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    centroids_all += [centroids]\n",
        "\n",
        "    ## plot the distribution of samples and the centroids\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(\"SafeDriver of data points\" \"(%d of KFold 10)\" %i, fontsize=18)\n",
        "    plt.grid(True)\n",
        "    plt.scatter(x_tr[kmeans.labels_ == 0, 2], x_tr[kmeans.labels_ == 0, 8],\n",
        "                c='purple', label='cluster 0')\n",
        "    plt.scatter(x_tr[kmeans.labels_ == 1, 2], x_tr[kmeans.labels_ == 1, 8],\n",
        "                c='yellow', label='cluster 1')\n",
        "    plt.scatter(centroids[0, 2], centroids[0, 8], marker='*', s=300, c='g', label='centroid 0')\n",
        "    plt.scatter(centroids[1, 2], centroids[1, 8], marker='*', s=300, c='r', label='centroid 1')\n",
        "    plt.legend()\n",
        "    plt.savefig('SafeDriver_Kmean_' + str(i) + 'of K fold 10.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    ## train and test the dataset in random forest model\n",
        "    modelRFnew = RandomForestClassifier(random_state=42)\n",
        "    modelRFnew.fit(x_tr,kmLabels)\n",
        "\n",
        "    percisionRFnew=precision_score(y_v,modelRFnew.predict(x_v))\n",
        "    recallRFnew=recall_score(y_v,modelRFnew.predict(x_v))\n",
        "    F1scoreRFnew=f1_score(y_v,modelRFnew.predict(x_v))\n",
        "    scoreRFRocnew = roc_auc_score(y_v,modelRFnew.predict(x_v))\n",
        "\n",
        "    cv_percisionRFnew.append(percisionRFnew)\n",
        "    cv_recallRFnew.append(recallRFnew)\n",
        "    cv_F1scoreRFnew.append(F1scoreRFnew)\n",
        "    cv_scoreRFRocnew.append(scoreRFRocnew)\n",
        "\n",
        "    print('Confusion matrix\\n',confusion_matrix(y_v,modelRFnew.predict(x_v)))\n",
        "    print('Precision:', precision_score(y_v,modelRFnew.predict(x_v)))\n",
        "    print('Recall:',recall_score(y_v,modelRFnew.predict(x_v)))\n",
        "    print('F1 score:', f1_score(y_v,modelRFnew.predict(x_v)))\n",
        "    print('ROC AUC score:',scoreRFRocnew)\n",
        "    \n",
        "    from sklearn.metrics import confusion_matrix    \n",
        "    ConfusionMatrix = confusion_matrix(y_v, modelRFnew.predict(x_v))\n",
        "    ConfusionMatrix_all += [ConfusionMatrix]\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu5qJ3_22olt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statistics import mean, stdev\n",
        "print(\"Mean of percision:\", round(mean(cv_percisionRFnew),3), '\\nStandard Deviation of percision:', round(stdev(cv_percisionRFnew),3))\n",
        "print(\"Mean of recall:\", round(mean(cv_recallRFnew),3), '\\nStandard Deviation of recall:', round(stdev(cv_recallRFnew),3))\n",
        "print(\"Mean of F1score:\", round(mean(cv_F1scoreRFnew),3), '\\nStandard Deviation of F1score:', round(stdev(cv_F1scoreRFnew),3))\n",
        "print(\"Mean of Roc score:\", round(mean(cv_scoreRFRocnew),3), '\\nStandard Deviation of Roc score:', round(stdev(cv_scoreRFRocnew),3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWzNvNw2KTfw",
        "colab_type": "text"
      },
      "source": [
        "##  8.Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXU_EeVMG48R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Ref:https://plotly.com/python/box-plots/\n",
        "## A boxplot of the cross-validation results for each method\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "x = ['Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision', 'Percision',\n",
        "     'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall',\n",
        "     'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score', 'F1 Score',\n",
        "     'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score', 'ROC-AUC Score']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=cv_percisionDT+cv_recallDT+cv_F1scoreDT+cv_scoreDTRoc,\n",
        "    x=x,\n",
        "    name='Decision Tree',\n",
        "    marker_color='#3D9970'\n",
        "))\n",
        "fig.add_trace(go.Box(\n",
        "    y=cv_percisionRF+cv_recallRF+cv_F1scoreRF+cv_scoreRFRoc,\n",
        "    x=x,\n",
        "    name='Random Forest',\n",
        "    marker_color='#FF4136'\n",
        "))\n",
        "fig.add_trace(go.Box(\n",
        "    y=cv_percisionRFnew+cv_recallRFnew+cv_F1scoreRFnew+cv_scoreRFRocnew,\n",
        "    x=x,\n",
        "    name='Mixture Method',\n",
        "    marker_color='#FF851B'\n",
        "))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    yaxis_title='Score',\n",
        "    boxmode='group' # group together boxes of the different traces for each value of x\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}